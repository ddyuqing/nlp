{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 今天是2020年1月05日，今天世界上又多了一名AI工程师 :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 作业截止时间\n",
    "此次作业截止时间为 2020.01.12日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：每道题是否回答完整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 自动驾驶、人脸识别、语音助理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 用git管理程序连接Github，管理文件的版本。jupyter对代码以及代码结果的可视化集成比较好，在学习上方便记录学习过程。pycharm就是ide啊，我用vscode并不知道为啥非要用pycharm。可能对python的编程环境管理得比较好吧，调试起来也比较方便？不需要安装额外的插件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:统计学习吧，我也不知道概率模型是啥。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:一些保险领域，其实就是利用概率模型定价的，如果能自适应得根据被保险人的各项数据更精准得预测发生事故得概率，就可以精准得对不同风险的人实施价格歧视，从而最有效达到资源分配。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:用概率可以衡量不同词语在文本中共同出现的可能性，进而用来作为计算机判断句子合不合一般规律的依据。第二问不知道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:根据语言模型可以判断一段随机生成的话出现的概率。根据连续词语共同出现的概率来判断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:聊天机器人、写作习惯模仿、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:P(w1w2w3w4)=p(w1|w2)p(w2|w3)p(w3|w4)P(w4)，假设了间隔1个以上的词语间联系很小，每个词只考虑跟它隔壁的词一起出现的可能性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:每个词只与隔壁的词有关联，如此算法计算量简化非常多，对于很局部的信息会有较好的结果。对于比较短的句子生成，或者提取局部的数据很有帮助。但是就完全忽略长程的关系，很可能句子一长，这个方法就会出现一些问题，比如“我是你哥哥，所以你是我爸爸。”这种话两段分开看没问题，但是一起看问题就出来了。\n",
    "而且感觉那个公式反了，应该是P(w1w2w3w4)=p(w1)p(w2|w1)p(w3|w2)p(w4|w3)，后面出现的词对前面的词的依赖会比较强，所以在算的时候，应该是假设w1先出现了，再有w2的概率比较合理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:P(w1w2w3w4)=p(w1)p(w2|w1)p(w3|w1w2)p(w4|w2w3)，假设了间隔2个以上的词语间联系很小，每个词只考虑跟它隔壁的词一起出现的可能性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1569578233461&di=4adfa7597fb380e7cc0e67190bbd7605&imgtype=0&src=http%3A%2F%2Fs1.sinaimg.cn%2Flarge%2F006eYYfyzy76cmpG3Yb1f)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammar(grammar_str, split='=>', line_split='\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split):\n",
    "        if not line.strip(): continue\n",
    "        exp, stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(gram, target):\n",
    "    if target not in gram: return target # means target is a terminal expression\n",
    "    \n",
    "    expaned = [generate(gram, t) for t in random.choice(gram[target])]\n",
    "    return ''.join([e if e != '/n' else '\\n' for e in expaned if e != 'null'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_first_grammar = \"\"\"\n",
    "navigation => subject instructions 就到 destination 了\n",
    "\n",
    "subject => null | 你 | 你们 | 各位 | 大家\n",
    "\n",
    "instructions => start path*\n",
    "start => directions action distance\n",
    "path* => null | landmarks directions action distance path* |landmarks directions action distance path*\n",
    "\n",
    "directions => 往前|靠右|靠左|右转|左转|回头|往上|向下\n",
    "action => 走| 行驶 | 直走\n",
    "distance => number quantifiers\n",
    "number => 一|两|三|四|五|六|七|八|九|十\n",
    "quantifiers => 步 | 米 | 百米 | 个路口 | 个红绿灯 | 个小区\n",
    "landmarks => ,然后| 再| , find_verb landmark\n",
    "find_verb => 看到| 遇到 |看见\n",
    "landmark => 红绿灯 | 十字路口 | destination\n",
    "\n",
    "destination => 7-11 | 麦当劳 | 便利店 | 天安门 | 地铁站 | 月球 | 火星 | 公司 | 太平洋\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar=create_grammar(my_first_grammar,split='=>',line_split='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'往前行驶六个路口再回头直走七个路口就到天安门了'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(gram=grammar, target='navigation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否提出了和课程上区别较大的语法结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_second_grammar = '''\n",
    "sentence_1 => 我是他 relation ，他是我 relation 。\n",
    "relation => 哥哥|姐姐|弟弟|妹妹\n",
    "\n",
    "sentence_2 => 我是他 last_sentence\n",
    "last_sentence => 哥哥 ， 他是我 弟弟或妹妹 。|姐姐 ， 他是我 弟弟或妹妹 。|弟弟 ， 他是我 哥哥或姐姐 。|妹妹 ， 他是我 哥哥或姐姐 。\n",
    "弟弟或妹妹 => 弟弟|妹妹\n",
    "哥哥或姐姐 => 哥哥|姐姐\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "会写这段的原因是，由于generate函数的限制，sentence_1生成的句子前后会没有逻辑关系。\n",
    "如果希望生成的句子，前后有一定的依赖关系，那语法的实现就会变得很复杂且冗长。\n",
    "虽然冗长的问题可以简单得透过语法中加入变量来解决。\n",
    "但也引出了下面的思考：\n",
    "\n",
    "句子的结构前后的逻辑应该由语法来定义吗？还是应该交给机器自己判断？\n",
    ">本来语言模型的功能，就是用来从一堆杂乱无章的句子集合中挑选出比较好的表达方式。正常如果完全不考虑计算资源的问题的话，让机器完全随机生成句子，不参考任何结构，这样的集合绝对可以包含所有的可能性。如果今天我们人为的定义了一些语法结构，或者是一定的逻辑依赖关系，其实就是在对这个集合加一定的限制条件，去掉一些我们觉得不可能的句型，减少不必要的生成。而这样的做法其实是有点违背数据驱动的理念的。同时再好的语法规则，也有可能有口语跟语法不符合的情况，所以怎么样在完备性与计算资源的节省间做取舍很重要。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：是否和上一个语法区别比较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(gram,target,n=1):\n",
    "    return [generate(gram,target) for i in range(n)]# you code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['向下行驶八百米,看到公司往上行驶五个路口再向下行驶九米再靠左走七个小区,遇到天安门靠右直走八个红绿灯就到地铁站了',\n '各位左转走一百米就到天安门了',\n '向下行驶两个红绿灯,看到十字路口左转走八个路口,看到月球回头行驶两个小区再向下行驶两个路口就到月球了',\n '你们靠右行驶十个小区再往前行驶一个小区就到月球了',\n '大家回头走九个红绿灯就到天安门了',\n '大家右转走七个小区就到月球了',\n '你们向下直走四个红绿灯就到麦当劳了',\n '回头走六步就到麦当劳了',\n '你们右转行驶四个路口,然后往上行驶六米就到麦当劳了',\n '你们靠左走十米再右转行驶五个红绿灯,然后左转走九个红绿灯,然后右转走八百米就到地铁站了']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar=create_grammar(my_first_grammar,split='=>',line_split='\\n')\n",
    "generate_n(gram=grammar,target='navigation',n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**; 运行代码，观察是否能够生成多个句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\ddyuqing\\.conda\\envs\\nlplearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>link</th>\n      <th>name</th>\n      <th>comment</th>\n      <th>star</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>https://movie.douban.com/subject/26363254/</td>\n      <td>战狼2</td>\n      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>https://movie.douban.com/subject/26363254/</td>\n      <td>战狼2</td>\n      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>https://movie.douban.com/subject/26363254/</td>\n      <td>战狼2</td>\n      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>https://movie.douban.com/subject/26363254/</td>\n      <td>战狼2</td>\n      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>https://movie.douban.com/subject/26363254/</td>\n      <td>战狼2</td>\n      <td>中二得很</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "  id                                        link name  \\\n0  1  https://movie.douban.com/subject/26363254/  战狼2   \n1  2  https://movie.douban.com/subject/26363254/  战狼2   \n2  3  https://movie.douban.com/subject/26363254/  战狼2   \n3  4  https://movie.douban.com/subject/26363254/  战狼2   \n4  5  https://movie.douban.com/subject/26363254/  战狼2   \n\n                                             comment star  \n0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n4                                               中二得很    1  "
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path='E:/nlp/lecture_material/lecture1/datasource-master/movie_comments.csv'\n",
    "content=pd.read_csv(file_path,encoding='utf-8')\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0                                   吴京意淫到了脑残的地步，看了恶心想吐\n1    首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...\n2    吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...\n3                        凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。\n4                                                 中二得很\nName: comment, dtype: object"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments=content['comment']\n",
    "comments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('article_9k.txt', 'w', encoding='utf-8') as f:\n",
    "    for comment in comments:\n",
    "        f.write(str(comment)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):\n",
    "    return list(jieba.cut(''.join(re.findall('\\w+',str(string)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\ddyuqing\\AppData\\Local\\Temp\\jieba.cache\n0\nLoading model cost 1.908 seconds.\nPrefix dict has been built succesfully.\n10000\n20000\n30000\n40000\n50000\n60000\n70000\n80000\n90000\n100000\n110000\n120000\n130000\n140000\n150000\n160000\n170000\n180000\n190000\n200000\n210000\n220000\n230000\n240000\n250000\n260000\n270000\n"
    }
   ],
   "source": [
    "TOKEN_1=[]\n",
    "for i, line in enumerate((open('article_9k.txt',encoding='utf-8'))):\n",
    "    if i % 10000 == 0: print(i)\n",
    "    \n",
    "    # replace 10000 with a big number when you do your homework. \n",
    "    \n",
    "    #if i > 100000: break    \n",
    "    TOKEN_1 += cut(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "4490666"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_1_counts=Counter(TOKEN_1)\n",
    "token_1_len=len(TOKEN_1)\n",
    "len(TOKEN_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_1(word):\n",
    "    if word in token_1_counts : return token_1_counts[word]/token_1_len\n",
    "    else:return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "4490664\n4490663\n"
    }
   ],
   "source": [
    "TOKEN_2_GRAM=[''.join(TOKEN_1[i:i+2]) for i in range(len(TOKEN_1[:-2]))]\n",
    "token_2_counts=Counter(TOKEN_2_GRAM)\n",
    "token_2_len=len(TOKEN_2_GRAM)\n",
    "TOKEN_3_GRAM=[''.join(TOKEN_1[i:i+3]) for i in range(len(TOKEN_1[:-3]))]\n",
    "token_3_counts=Counter(TOKEN_3_GRAM)\n",
    "token_3_len=len(TOKEN_3_GRAM)\n",
    "print(token_2_len)\n",
    "print(token_3_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[('的', 328265), ('了', 102421), ('是', 73098), ('我', 50330), ('都', 36255), ('很', 34711), ('看', 34028), ('电影', 33675), ('也', 32065), ('和', 31290)]\n[('的电影', 8640), ('看的', 7106), ('都是', 6335), ('让人', 5283), ('的故事', 4709), ('看了', 4585), ('也是', 4408), ('的时候', 4398), ('的人', 4357), ('的是', 4348)]\n[('看完了', 699), ('每个人都', 621), ('最喜欢的', 598), ('挺好的', 564), ('看的时候', 537), ('很好的', 519), ('的一部电影', 451), ('的都是', 425), ('挺好看的', 417), ('还是不错的', 417)]\n"
    }
   ],
   "source": [
    "print(token_1_counts.most_common()[:10])\n",
    "print(token_2_counts.most_common()[:10])\n",
    "print(token_3_counts.most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_2(word1,word2):\n",
    "    if (word1 + word2 in token_2_counts) & (word2 in token_1_counts): return token_2_counts[word1+word2] / token_1_counts[word2]\n",
    "    else:\n",
    "        return 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_3(word1,word2,word3):\n",
    "    if (word1+word2+word3 in token_3_counts) & (word2 + word3 in token_2_counts): return token_3_counts[word1+word2+word3]/token_2_counts[word2+word3]\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probablity(sentence,lang_model):\n",
    "    words = cut(sentence)\n",
    "    sentence_pro = 1\n",
    "    if lang_model == '1_gram':\n",
    "        sentence_pro *= prob_1(words[0])\n",
    "        for i, word in enumerate(words[:-1]):\n",
    "            next_1 = words[i+1]\n",
    "            \n",
    "            probability = prob_2(next_1, word)\n",
    "            #print(probability)\n",
    "            sentence_pro *= probability\n",
    "        if sentence_pro == 1 : sentence_pro=0\n",
    "    elif lang_model == '2_gram':\n",
    "        sentence_pro *= prob_1(words[0])*prob_2(words[1],words[0])\n",
    "        for i, word in enumerate(words[:-2]):\n",
    "            next_1 = words[i+1]\n",
    "            next_2 = words[i+2]\n",
    "            probability = prob_3(next_2, next_1, word)\n",
    "            #print(probability)\n",
    "            sentence_pro *= probability\n",
    "        if sentence_pro == 1 : sentence_pro=0\n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.2268411856949502e-07"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('你往前走六米就到地铁站了','2_gram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['你们往前行驶十百米,然后往前行驶十个路口再靠左行驶六百米就到便利店了',\n '靠左走九个红绿灯,然后靠右直走七个小区就到便利店了',\n '你左转走两个红绿灯就到月球了',\n '各位靠右行驶五步,然后左转行驶八个红绿灯,看见红绿灯右转直走七个路口就到公司了',\n '你靠右行驶两百米,然后靠右直走四个小区就到麦当劳了',\n '回头走一米就到公司了',\n '大家右转走十百米,然后向下直走四个小区,然后向下直走三米,然后右转行驶八个路口,然后靠右走一步,然后右转行驶四百米,然后左转行驶八个小区就到公司了',\n '各位向下行驶四步就到公司了',\n '你靠左直走八个小区,看见红绿灯往上直走五米,看到红绿灯靠右直走两个红绿灯,然后往前行驶两米再右转走三米,看见红绿灯往前走七步,然后右转直走八米,然后靠左行驶九个小区,看到十字路口右转行驶十米,然后靠左行驶两个路口,遇到便利店向下行驶八米再回头走十个红绿灯,看见月球靠右走一百米,然后靠左走六米就到月球了',\n '大家回头走九个小区,遇到便利店回头行驶八个红绿灯,然后右转走三个路口,然后回头行驶四个路口,看到红绿灯往上走六个红绿灯再向下走十步,看见太平洋靠右行驶五步,遇到太平洋往上直走九米再左转走八米就到便利店了']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar=create_grammar(my_first_grammar,split='=>',line_split='\\n')\n",
    "generate_n(gram=grammar,target='navigation',n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "sentence: 你们回头行驶四个路口,看见红绿灯往前直走一步再靠左行驶九个小区就到火星了 with Prb: 0.0004023902022550775\nsentence: 大家向下行驶九个小区再右转直走六百米就到公司了 with Prb: 0.00030329576949165223\nsentence: 大家往前行驶八百米就到天安门了 with Prb: 0.00030329576949165223\nsentence: 往上行驶八百米,然后靠右行驶四个红绿灯就到太平洋了 with Prb: 2.2268411856949502e-07\nsentence: 你回头行驶五个小区就到7-11了 with Prb: 2.2268411856949502e-07\nsentence: 你们靠左直走一个红绿灯,遇到十字路口靠左直走八个路口,看见火星靠左走十个路口,遇到天安门靠左走九米,看见7-11左转直走两步,然后左转走四米,看见7-11向下行驶八个红绿灯就到公司了 with Prb: 0.0004023902022550775\nsentence: 回头直走七个路口,遇到十字路口回头行驶五个红绿灯就到公司了 with Prb: 5.722981847236022e-05\nsentence: 大家左转直走三个红绿灯就到天安门了 with Prb: 0.00030329576949165223\nsentence: 大家右转行驶七个红绿灯,然后右转直走一个小区再右转直走六个路口再靠右走八个路口,看见十字路口往前直走九个小区,看见太平洋回头行驶十米再回头行驶九步就到7-11了 with Prb: 0.00030329576949165223\nsentence: 大家向下行驶十百米,然后靠左直走四个小区就到天安门了 with Prb: 0.00030329576949165223\n"
    }
   ],
   "source": [
    "for sentence in generate_n(gram=grammar,target='navigation',n=10):\n",
    "    print('sentence: {} with Prb: {}'.format(sentence, get_probablity(sentence,'2_gram')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点** 1. 是否使用了新的数据集； 2. csv(txt)数据是否正确解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(gram,target,lang_model,gen_number):\n",
    "    result_list=[]\n",
    "    for sentence in generate_n(gram,target,gen_number):\n",
    "        result_list.append((sentence,get_probablity(sentence,lang_model)))\n",
    "    result_list.sort(key=lambda x: x[1],reverse=True)\n",
    "    #print(result_list)\n",
    "    return result_list[0][0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'你往前行驶八步就到月球了'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar=create_grammar(my_first_grammar,split='=>',line_split='\\n')\n",
    "generate_best(gram=grammar,target='navigation',lang_model='2_gram',gen_number=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 3, 5]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(1, 4), (2, 5), (4, 4), (5, 0)]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(5, 0), (1, 4), (4, 4), (2, 5)]"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(2, 5), (1, 4), (4, 4), (5, 0)]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否使用 lambda 语法进行排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "数据量不大的情况下，很难包含所生成的词语情况，2gram要利用到三个词同时出现，包含的概率又会下降很多。3gram就跟不用说了。只要无法连续得出现数据中得词语，那在这个模型下很容易就落入1/len(token)这个极小的概率中，如此句子越长概率就越来越低了，而且差不多长的句子可能概率还一样。这部分可能可以将没在数据中出现的词语链的概率设之成1，或者直接不考虑没出现在数据库中的词汇，使得模型不会因为出现数据库不包含的词的概率太高而收敛到0。从而只关注那些有出现在数据库中的词。\n",
    "\n",
    "好吧我尝试了一下，如果不在数据库中的词汇真的太多，将概率设为1的方法也于事无补，最后就是有出现词库中词语的句子获胜而已。因此又想到一个办法，虽然这些词没有出现过，但是他们还是会有跟词库里的词有相似的特征，比如词性、发音、词语类别等等。把句子再抽象成不同词性或词语类别的组合，来做语言模型的判断，综合考量，可能就可以在词语不在数据库中的情况下作为两个词可不可能一起出现的参考。不过jieba分词好像做不到，nlpir可以，实在没时间改了，有机会再试试吧。\n",
    "\n",
    "另外有些时候词与词之间的联系可以是中长程的，这时候这种假设词语只跟附近的词有关系的模型就会失效。这个问题，可能可以综合考量不同gram数的概率结果，再将他们乘起来，或许就能包含长程和短程的讯息了呢。长程短程都高>其中一个高>两个都很低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**评阅点**: 是否提出了比较实际的问题，例如OOV问题，例如数据量，例如变成 3-gram问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}